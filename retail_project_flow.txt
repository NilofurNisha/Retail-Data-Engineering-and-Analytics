Project Flow:
=============

1. Sqoop for injecting the data from different databases/schemas using most of the options.
2. HDFS for persisting the data for primary staging.
3. Hive to parse
   a. Convert to complex data type, reduce/change the order of columns extracted from the database,
      Filter the custdetails complex data which has non zero amounts.
   b. Split based on the amount of the product, reorder the columns, union and store the output into
      HDFS.
4. Hive to create and load ..
   a. Under staging Database create managed temporary tables, Load the Sqoop imported data into
      the above managed tables that will be dropped and recreated on daily basis.
   b. Under retail mart database create and load external tables which is partitioned for applying
      where clauses and bucketed to join with the other staging tables.
   c. Index the high cardinal values.
   d. Create a final external table which is partitioned and not bucketed.
   e. Customer Website viewship pattern and Frustration Scoring use case queries.
   f. Create a hive â€“ hbase handler table to load the refined data into hbase.
5. Sqoop Export to load the aggregated data to RDBMS again .
6. HBase to view the data generated in the hive handler table.
7. Phoenix to create a table on top of hbase table to perform low latency queries. 